# BitNet CLI

[![Crates.io](https://img.shields.io/crates/v/bitnet-cli.svg)](https://crates.io/crates/bitnet-cli)
[![Documentation](https://docs.rs/bitnet-cli/badge.svg)](https://docs.rs/bitnet-cli)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](../LICENSE)

Command-line interface for BitNet neural networks, providing tools for model conversion, inference, training, benchmarking, and profiling.

## ğŸ¯ Purpose

`bitnet-cli` provides a comprehensive command-line interface for BitNet operations:

- **Model Operations**: Convert, quantize, and optimize models
- **Inference Tools**: Run inference with various configurations
- **Training Commands**: Train and fine-tune BitNet models
- **Benchmarking**: Performance benchmarking and profiling
- **Utilities**: Model analysis, validation, and debugging tools

## ğŸ”´ Current Status: **PLACEHOLDER ONLY**

âš ï¸ **This crate is currently a placeholder and contains no implementation.**

The current `src/main.rs` contains only:
```rust
//! BitNet CLI Application
//! 
//! Command-line interface for BitNet operations.

fn main() {
    println!("BitNet CLI - Coming Soon!");
}
```

## âœ… What Needs to be Implemented

### ğŸ”´ **Model Management Commands** (Not Implemented)

#### Model Conversion
- **Format Conversion**: Convert between different model formats (SafeTensors, ONNX, PyTorch)
- **Quantization**: Convert FP32/FP16 models to BitNet 1.58-bit format
- **Optimization**: Apply graph optimizations and operator fusion
- **Validation**: Validate converted models for correctness

#### Model Analysis
- **Model Info**: Display model architecture, parameters, and memory usage
- **Layer Analysis**: Analyze individual layers and their properties
- **Quantization Analysis**: Analyze quantization quality and accuracy loss
- **Performance Profiling**: Profile model performance characteristics

#### Model Utilities
- **Model Comparison**: Compare different model versions and formats
- **Model Merging**: Merge LoRA adapters with base models
- **Model Splitting**: Split large models for distributed inference
- **Model Compression**: Apply additional compression techniques

### ğŸ”´ **Inference Commands** (Not Implemented)

#### Interactive Inference
- **Chat Mode**: Interactive chat interface for language models
- **Completion Mode**: Text completion with various sampling strategies
- **Batch Inference**: Process multiple inputs efficiently
- **Streaming Inference**: Real-time streaming text generation

#### Inference Configuration
- **Device Selection**: Choose between CPU, GPU, and Neural Engine
- **Performance Tuning**: Optimize inference for speed or memory
- **Quantization Settings**: Configure runtime quantization parameters
- **Generation Parameters**: Control temperature, top-k, top-p, etc.

#### Inference Utilities
- **Benchmark Inference**: Measure inference performance
- **Memory Profiling**: Profile memory usage during inference
- **Accuracy Testing**: Test model accuracy on datasets
- **Latency Analysis**: Analyze inference latency characteristics

### ğŸ”´ **Training Commands** (Not Implemented)

#### Training Management
- **Start Training**: Launch training jobs with various configurations
- **Resume Training**: Resume interrupted training from checkpoints
- **Monitor Training**: Monitor training progress and metrics
- **Stop Training**: Gracefully stop training jobs

#### Fine-Tuning
- **LoRA Fine-tuning**: Fine-tune models with LoRA adapters
- **QLoRA Fine-tuning**: Memory-efficient fine-tuning with QLoRA
- **Full Fine-tuning**: Traditional full model fine-tuning
- **Custom Fine-tuning**: Custom fine-tuning strategies

#### Training Utilities
- **Dataset Preparation**: Prepare and validate training datasets
- **Hyperparameter Tuning**: Automated hyperparameter optimization
- **Training Analysis**: Analyze training metrics and convergence
- **Model Evaluation**: Evaluate trained models on test sets

### ğŸ”´ **Benchmarking and Profiling** (Not Implemented)

#### Performance Benchmarking
- **Inference Benchmarks**: Comprehensive inference performance testing
- **Training Benchmarks**: Training performance and scaling tests
- **Memory Benchmarks**: Memory usage and efficiency tests
- **Throughput Benchmarks**: Measure tokens per second and batch throughput

#### System Profiling
- **Hardware Profiling**: Profile CPU, GPU, and memory usage
- **Thermal Profiling**: Monitor thermal characteristics during operation
- **Power Profiling**: Measure power consumption (on supported platforms)
- **Network Profiling**: Profile distributed training communication

#### Comparative Analysis
- **Model Comparison**: Compare different models and configurations
- **Hardware Comparison**: Compare performance across different hardware
- **Configuration Comparison**: Compare different runtime configurations
- **Historical Analysis**: Track performance changes over time

## ğŸš€ Planned CLI Interface

### Model Operations

```bash
# Convert model formats
bitnet model convert --input model.pytorch --output model.safetensors --format safetensors

# Quantize model to BitNet format
bitnet model quantize --input model.safetensors --output model_bitnet.safetensors --bits 1.58

# Analyze model
bitnet model info model.safetensors
bitnet model analyze --detailed model.safetensors

# Optimize model
bitnet model optimize --input model.safetensors --output optimized.safetensors --target apple-silicon
```

### Inference Operations

```bash
# Interactive chat
bitnet chat --model model.safetensors --device auto

# Text completion
bitnet complete --model model.safetensors --prompt "The future of AI is" --max-length 100

# Batch inference
bitnet infer --model model.safetensors --input prompts.txt --output results.txt --batch-size 32

# Streaming inference
bitnet stream --model model.safetensors --prompt "Tell me a story" --stream-tokens
```

### Training Operations

```bash
# Start training
bitnet train --model base_model.safetensors --dataset dataset.jsonl --config training_config.yaml

# LoRA fine-tuning
bitnet finetune lora --model model.safetensors --dataset dataset.jsonl --rank 16 --alpha 32

# QLoRA fine-tuning
bitnet finetune qlora --model model.safetensors --dataset dataset.jsonl --bits 4

# Resume training
bitnet train resume --checkpoint checkpoint_1000.pt
```

### Benchmarking Operations

```bash
# Benchmark inference
bitnet benchmark inference --model model.safetensors --batch-sizes 1,8,32 --sequence-lengths 512,1024,2048

# Benchmark training
bitnet benchmark training --model model.safetensors --dataset dataset.jsonl --batch-sizes 8,16,32

# System profiling
bitnet profile system --model model.safetensors --duration 60s --output profile.json

# Compare models
bitnet compare models model1.safetensors model2.safetensors --metric throughput,memory,accuracy
```

### Utility Operations

```bash
# Validate model
bitnet validate --model model.safetensors --test-dataset test.jsonl

# Model diagnostics
bitnet diagnose --model model.safetensors --verbose

# Configuration management
bitnet config show
bitnet config set device.default gpu
bitnet config reset

# Help and documentation
bitnet help
bitnet help train
bitnet --version
```

## ğŸ—ï¸ Planned Architecture

### CLI Structure

```
bitnet-cli/src/
â”œâ”€â”€ main.rs                  # Main CLI entry point
â”œâ”€â”€ cli/                     # CLI interface and parsing
â”‚   â”œâ”€â”€ mod.rs              # CLI module interface
â”‚   â”œâ”€â”€ app.rs              # Main CLI application
â”‚   â”œâ”€â”€ commands/           # Command implementations
â”‚   â”‚   â”œâ”€â”€ mod.rs          # Commands interface
â”‚   â”‚   â”œâ”€â”€ model.rs        # Model management commands
â”‚   â”‚   â”œâ”€â”€ inference.rs    # Inference commands
â”‚   â”‚   â”œâ”€â”€ training.rs     # Training commands
â”‚   â”‚   â”œâ”€â”€ benchmark.rs    # Benchmarking commands
â”‚   â”‚   â”œâ”€â”€ profile.rs      # Profiling commands
â”‚   â”‚   â”œâ”€â”€ config.rs       # Configuration commands
â”‚   â”‚   â””â”€â”€ utils.rs        # Utility commands
â”‚   â”œâ”€â”€ args/               # Command-line argument parsing
â”‚   â”‚   â”œâ”€â”€ mod.rs          # Args interface
â”‚   â”‚   â”œâ”€â”€ model_args.rs   # Model command arguments
â”‚   â”‚   â”œâ”€â”€ inference_args.rs # Inference arguments
â”‚   â”‚   â”œâ”€â”€ training_args.rs # Training arguments
â”‚   â”‚   â””â”€â”€ common_args.rs  # Common arguments
â”‚   â””â”€â”€ output/             # Output formatting
â”‚       â”œâ”€â”€ mod.rs          # Output interface
â”‚       â”œâ”€â”€ formatters.rs   # Output formatters
â”‚       â”œâ”€â”€ progress.rs     # Progress indicators
â”‚       â””â”€â”€ tables.rs       # Table formatting
â”œâ”€â”€ config/                  # Configuration management
â”‚   â”œâ”€â”€ mod.rs              # Config interface
â”‚   â”œâ”€â”€ settings.rs         # Application settings
â”‚   â”œâ”€â”€ profiles.rs         # Configuration profiles
â”‚   â”œâ”€â”€ validation.rs       # Config validation
â”‚   â””â”€â”€ migration.rs        # Config migration
â”œâ”€â”€ operations/              # Core operations
â”‚   â”œâ”€â”€ mod.rs              # Operations interface
â”‚   â”œâ”€â”€ model_ops.rs        # Model operations
â”‚   â”œâ”€â”€ inference_ops.rs    # Inference operations
â”‚   â”œâ”€â”€ training_ops.rs     # Training operations
â”‚   â”œâ”€â”€ benchmark_ops.rs    # Benchmarking operations
â”‚   â””â”€â”€ profile_ops.rs      # Profiling operations
â”œâ”€â”€ interactive/             # Interactive modes
â”‚   â”œâ”€â”€ mod.rs              # Interactive interface
â”‚   â”œâ”€â”€ chat.rs             # Chat interface
â”‚   â”œâ”€â”€ repl.rs             # REPL interface
â”‚   â”œâ”€â”€ wizard.rs           # Configuration wizard
â”‚   â””â”€â”€ monitor.rs          # Training monitor
â”œâ”€â”€ utils/                   # CLI utilities
â”‚   â”œâ”€â”€ mod.rs              # Utils interface
â”‚   â”œâ”€â”€ logging.rs          # Logging setup
â”‚   â”œâ”€â”€ error_handling.rs   # Error handling
â”‚   â”œâ”€â”€ file_utils.rs       # File utilities
â”‚   â”œâ”€â”€ system_info.rs      # System information
â”‚   â””â”€â”€ validation.rs       # Input validation
â””â”€â”€ integrations/            # External integrations
    â”œâ”€â”€ mod.rs              # Integrations interface
    â”œâ”€â”€ tensorboard.rs      # TensorBoard integration
    â”œâ”€â”€ wandb.rs            # Weights & Biases
    â”œâ”€â”€ mlflow.rs           # MLflow integration
    â””â”€â”€ huggingface.rs      # Hugging Face Hub
```

### Command Structure

```rust
// Example command structure
use clap::{Parser, Subcommand};

#[derive(Parser)]
#[command(name = "bitnet")]
#[command(about = "BitNet neural network toolkit")]
pub struct Cli {
    #[command(subcommand)]
    pub command: Commands,
    
    #[arg(long, global = true)]
    pub verbose: bool,
    
    #[arg(long, global = true)]
    pub config: Option<PathBuf>,
}

#[derive(Subcommand)]
pub enum Commands {
    /// Model management operations
    Model {
        #[command(subcommand)]
        action: ModelCommands,
    },
    /// Inference operations
    Infer(InferenceArgs),
    /// Training operations
    Train(TrainingArgs),
    /// Benchmarking operations
    Benchmark {
        #[command(subcommand)]
        benchmark_type: BenchmarkCommands,
    },
    /// Configuration management
    Config {
        #[command(subcommand)]
        action: ConfigCommands,
    },
}
```

## ğŸ“Š Expected Features and Performance

### User Experience Features

| Feature | Description | Priority |
|---------|-------------|----------|
| **Interactive Chat** | Real-time chat interface | High |
| **Progress Indicators** | Visual progress for long operations | High |
| **Auto-completion** | Shell auto-completion support | Medium |
| **Configuration Wizard** | Guided setup for new users | Medium |
| **Rich Output** | Colored and formatted output | Medium |

### Performance Characteristics

| Operation | Expected Performance | Memory Usage |
|-----------|---------------------|--------------|
| **Model Loading** | <5s for 7B model | <1GB overhead |
| **Inference (single)** | <200ms latency | <4GB total |
| **Inference (batch)** | >100 tok/s | <8GB total |
| **Model Conversion** | >1GB/s throughput | <2x model size |

### Platform Support

| Platform | Support Level | Features |
|----------|---------------|----------|
| **macOS (Apple Silicon)** | Full | All features, Metal acceleration |
| **macOS (Intel)** | Full | All features, CPU only |
| **Linux (x86_64)** | Full | All features, CUDA support |
| **Windows** | Partial | Basic features, CPU only |

## ğŸ§ª Planned Testing Strategy

### Unit Tests
```bash
# Test CLI argument parsing
cargo test --package bitnet-cli cli

# Test command implementations
cargo test --package bitnet-cli commands

# Test configuration management
cargo test --package bitnet-cli config
```

### Integration Tests
```bash
# Test end-to-end workflows
cargo test --package bitnet-cli --test e2e_workflows

# Test model operations
cargo test --package bitnet-cli --test model_operations

# Test inference operations
cargo test --package bitnet-cli --test inference_operations
```

### CLI Tests
```bash
# Test CLI interface
cargo test --package bitnet-cli --test cli_interface

# Test interactive modes
cargo test --package bitnet-cli --test interactive_modes

# Test error handling
cargo test --package bitnet-cli --test error_handling
```

### User Acceptance Tests
```bash
# Test user workflows
cargo test --package bitnet-cli --test user_workflows

# Test documentation examples
cargo test --package bitnet-cli --test doc_examples

# Test performance benchmarks
cargo bench --package bitnet-cli
```

## ğŸ”§ Configuration

### Global Configuration

```yaml
# ~/.bitnet/config.yaml
device:
  default: "auto"
  fallback: ["cpu"]
  memory_fraction: 0.8

inference:
  default_batch_size: 1
  max_sequence_length: 2048
  temperature: 0.8
  top_k: 50
  top_p: 0.9

training:
  default_learning_rate: 1e-4
  default_batch_size: 8
  checkpoint_interval: 1000
  log_interval: 100

output:
  format: "auto"
  color: true
  progress_bars: true
  verbosity: "info"

paths:
  models_dir: "~/.bitnet/models"
  cache_dir: "~/.bitnet/cache"
  logs_dir: "~/.bitnet/logs"
```

### Command-Specific Configuration

```yaml
# training_config.yaml
model:
  base_model: "microsoft/DialoGPT-medium"
  quantization:
    bits: 1.58
    calibration_samples: 512

training:
  learning_rate: 5e-5
  batch_size: 16
  num_epochs: 3
  warmup_steps: 500
  
  optimizer:
    type: "adamw"
    weight_decay: 0.01
    
  scheduler:
    type: "cosine"
    warmup_ratio: 0.1

data:
  train_file: "train.jsonl"
  validation_file: "val.jsonl"
  max_length: 1024
  
logging:
  wandb:
    project: "bitnet-finetuning"
    entity: "my-team"
```

## ğŸš€ Installation and Usage

### Installation

```bash
# Install from crates.io (when published)
cargo install bitnet-cli

# Install from source
git clone https://github.com/bitnet-rust/bitnet-rust.git
cd bitnet-rust
cargo install --path bitnet-cli

# Install with all features
cargo install bitnet-cli --features "metal,cuda,distributed"
```

### Shell Completion

```bash
# Generate shell completions
bitnet completion bash > ~/.bash_completion.d/bitnet
bitnet completion zsh > ~/.zsh/completions/_bitnet
bitnet completion fish > ~/.config/fish/completions/bitnet.fish

# Or install via package managers
brew install bitnet-cli  # macOS
apt install bitnet-cli   # Ubuntu/Debian
```

### Quick Start

```bash
# Initialize configuration
bitnet config init

# Download a model
bitnet model download microsoft/DialoGPT-medium

# Convert to BitNet format
bitnet model quantize microsoft/DialoGPT-medium --output bitnet-dialog.safetensors

# Start interactive chat
bitnet chat bitnet-dialog.safetensors

# Run benchmarks
bitnet benchmark inference bitnet-dialog.safetensors
```

## ğŸ¯ User Experience Goals

### Ease of Use
- **Intuitive Commands**: Natural language-like command structure
- **Helpful Defaults**: Sensible defaults for all operations
- **Clear Error Messages**: Actionable error messages with suggestions
- **Progressive Disclosure**: Simple commands with advanced options

### Performance
- **Fast Startup**: CLI should start quickly (<100ms)
- **Efficient Operations**: Minimize overhead for all operations
- **Parallel Processing**: Utilize multiple cores when possible
- **Memory Efficiency**: Minimize memory usage for CLI operations

### Reliability
- **Robust Error Handling**: Graceful handling of all error conditions
- **Input Validation**: Comprehensive validation of user inputs
- **Safe Operations**: Prevent destructive operations without confirmation
- **Recovery**: Ability to recover from interrupted operations

## ğŸ¤ Contributing

This crate needs complete implementation! Priority areas:

1. **CLI Framework**: Build the basic CLI structure and argument parsing
2. **Model Operations**: Implement model conversion and analysis commands
3. **Inference Interface**: Create interactive and batch inference commands
4. **Training Commands**: Add training and fine-tuning command support

### Getting Started

1. Study CLI design patterns and user experience principles
2. Implement basic CLI structure with clap
3. Add model loading and conversion commands
4. Implement interactive chat interface
5. Add comprehensive help and documentation

### Development Guidelines

1. **User-Centric Design**: Focus on user experience and ease of use
2. **Comprehensive Testing**: Test all CLI interactions and edge cases
3. **Clear Documentation**: Provide clear help text and examples
4. **Performance**: Optimize for fast startup and efficient operations

## ğŸ“š References

- **CLI Design**: [Command Line Interface Guidelines](https://clig.dev/)
- **Clap Documentation**: [Clap Command Line Parser](https://docs.rs/clap/)
- **User Experience**: [The Art of Command Line](https://github.com/jlevy/the-art-of-command-line)
- **BitNet Paper**: [BitNet: Scaling 1-bit Transformers](https://arxiv.org/abs/2310.11453)

## ğŸ“„ License

Licensed under the MIT License. See [LICENSE](../LICENSE) for details.