//! Day 5 Memory Management Optimization Demonstration
//!
//! This example demonstrates the advanced GPU memory management and enhanced memory pooling
//! capabilities implemented in Phase 5 Day 5 of the BitNet-Rust inference engine development.

use bitnet_inference::{
    engine::gpu_memory_optimizer::GPUMemoryManager,
    cache::enhanced_memory_pool::{EnhancedMemoryPool, AllocationStrategy},
    api::{InferenceEngine, EngineConfig},
    engine::OptimizationLevel,
};
use bitnet_core::{Device, Tensor, DType};
use std::time::Instant;
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    println!("ðŸš€ BitNet Inference Engine - Day 5 Memory Management Optimization Demo");
    println!("=======================================================================\n");

    // Step 1: Demonstrate GPU Memory Optimization
    println!("ðŸ“Š Step 1: GPU Memory Optimization");
    println!("-----------------------------------");
    
    if let Err(e) = demonstrate_gpu_memory_optimization().await {
        println!("âš ï¸  GPU memory optimization demo failed (this is expected without actual GPU): {}", e);
    }
    
    println!();

    // Step 2: Enhanced Memory Pool Demonstration
    println!("ðŸŠ Step 2: Enhanced Memory Pool with Cross-Backend Cache");
    println!("-------------------------------------------------------");
    
    demonstrate_enhanced_memory_pool().await?;
    println!();

    // Step 3: Memory Allocation Strategy Comparison
    println!("ðŸŽ¯ Step 3: Memory Allocation Strategy Comparison");
    println!("-----------------------------------------------");
    
    demonstrate_allocation_strategies().await?;
    println!();

    // Step 4: Memory Transfer Optimization
    println!("ðŸ”„ Step 4: Cross-Device Memory Transfer Optimization");
    println!("---------------------------------------------------");
    
    demonstrate_memory_transfers().await?;
    println!();

    // Step 5: Performance Impact Analysis
    println!("ðŸ“ˆ Step 5: Memory Optimization Performance Impact");
    println!("------------------------------------------------");
    
    demonstrate_performance_impact().await?;

    println!("\nâœ… Day 5 Memory Management Optimization demonstration completed successfully!");
    Ok(())
}

/// Demonstrate GPU memory optimization capabilities
async fn demonstrate_gpu_memory_optimization() -> Result<()> {
    println!("ðŸ”§ Creating GPU Memory Manager for different backends...");
    
    // Test CPU backend (always available)
    let cpu_manager_result = GPUMemoryManager::new(Device::Cpu);
    match cpu_manager_result {
        Ok(manager) => {
            let stats = manager.get_memory_stats()?;
            println!("âœ… CPU Memory Manager created successfully");
            println!("   - Total allocated: {} bytes", stats.total_allocated);
            println!("   - Active allocations: {}", stats.active_allocations);
        }
        Err(e) => println!("âŒ CPU Memory Manager failed: {}", e),
    }

    // Test Metal backend (if available)
    #[cfg(feature = "metal")]
    {
        println!("ðŸ”§ Attempting Metal Memory Manager creation...");
        
        // Create a Metal device for testing
        // Since MetalDevice::new() isn't available, we'll use the CPU as fallback
        // In a real scenario, Metal device initialization would be handled by candle-core
        let cpu_fallback = GPUMemoryManager::new(Device::Cpu);
        
        match cpu_fallback {
            Ok(mut manager) => {
                println!("âœ… Memory Manager created successfully (CPU fallback for Metal test)");
                
                // Demonstrate buffer optimization
                if let Ok(()) = manager.optimize_all() {
                    println!("   - Buffer pool optimization completed");
                    
                    let stats = manager.get_memory_stats()?;
                    println!("   - Buffer pool hit rate: {:.2}%", stats.buffer_pool_hit_rate);
                    println!("   - Fragmentation: {:.2}%", stats.fragmentation_percentage);
                }
            }
            Err(e) => println!("âš ï¸  Memory Manager not available: {}", e),
        }
    }

    // Note: MLX backend testing would be similar but using Device::Cpu as fallback
    // since MLX devices aren't easily constructable in this demo context
    println!("â„¹ï¸  MLX backend would be tested similarly but requires actual MLX device initialization");

    Ok(())
}

/// Demonstrate enhanced memory pool capabilities
async fn demonstrate_enhanced_memory_pool() -> Result<()> {
    println!("ðŸ—ï¸  Creating Enhanced Memory Pool...");
    
    let pool_result = EnhancedMemoryPool::new(
        Device::Cpu,
        512 * 1024 * 1024, // 512MB CPU pool
        128 * 1024 * 1024  // 128MB cross-backend cache
    );
    
    match pool_result {
        Ok(mut pool) => {
            println!("âœ… Enhanced Memory Pool created successfully");
            
            // Allocate some memory regions
            println!("ðŸ“¦ Allocating memory regions...");
            
            let regions = vec![
                pool.allocate_optimal(1024 * 1024, Device::Cpu)?,      // 1MB
                pool.allocate_optimal(4 * 1024 * 1024, Device::Cpu)?,  // 4MB
                pool.allocate_optimal(16 * 1024 * 1024, Device::Cpu)?, // 16MB
            ];
            
            println!("   - Allocated {} memory regions", regions.len());
            for (i, region) in regions.iter().enumerate() {
                println!("   - Region {}: {} bytes on {:?}", i + 1, region.size, region.device);
            }
            
            // Demonstrate memory optimization
            println!("âš¡ Optimizing memory pool...");
            pool.optimize_all()?;
            
            // Get comprehensive statistics
            let stats = pool.get_comprehensive_stats()?;
            println!("ðŸ“Š Memory Pool Statistics:");
            println!("{}", stats.generate_report());
            
            // Deallocate regions
            println!("ðŸ—‘ï¸  Deallocating memory regions...");
            for region in regions {
                pool.deallocate_region(region)?;
            }
            println!("   - All regions deallocated successfully");
        }
        Err(e) => println!("âŒ Enhanced Memory Pool creation failed: {}", e),
    }
    
    Ok(())
}

/// Demonstrate different allocation strategies
async fn demonstrate_allocation_strategies() -> Result<()> {
    println!("ðŸŽ® Testing different allocation strategies...");
    
    let pool_result = EnhancedMemoryPool::new(Device::Cpu, 256 * 1024 * 1024, 64 * 1024 * 1024);
    
    match pool_result {
        Ok(mut pool) => {
            let strategies = vec![
                (AllocationStrategy::PreferCPU, "Prefer CPU"),
                (AllocationStrategy::PreferGPU, "Prefer GPU"),
                (AllocationStrategy::Automatic, "Automatic"),
                (AllocationStrategy::MinimizeTransfers, "Minimize Transfers"),
            ];
            
            for (strategy, name) in strategies {
                println!("ðŸ”§ Testing {} strategy:", name);
                pool.set_allocation_strategy(strategy);
                
                let start = Instant::now();
                let mut regions = Vec::new();
                
                // Allocate test regions
                for size in [1024, 4096, 16384, 65536] { // Various sizes
                    match pool.allocate_optimal(size, Device::Cpu) {
                        Ok(region) => {
                            regions.push(region);
                        }
                        Err(e) => {
                            println!("   âš ï¸  Allocation failed for size {}: {}", size, e);
                            continue;
                        }
                    }
                }
                
                let allocation_time = start.elapsed();
                println!("   - Allocated {} regions in {:?}", regions.len(), allocation_time);
                
                // Get statistics
                if let Ok(stats) = pool.get_comprehensive_stats() {
                    println!("   - Memory efficiency: {:.1}%", stats.global_stats.memory_efficiency);
                }
                
                // Clean up
                for region in regions {
                    pool.deallocate_region(region)?;
                }
            }
        }
        Err(e) => println!("âŒ Enhanced Memory Pool creation failed: {}", e),
    }
    
    Ok(())
}

/// Demonstrate cross-device memory transfers
async fn demonstrate_memory_transfers() -> Result<()> {
    println!("ðŸ“¤ Demonstrating cross-device memory transfers...");
    
    let pool_result = EnhancedMemoryPool::new(Device::Cpu, 256 * 1024 * 1024, 64 * 1024 * 1024);
    
    match pool_result {
        Ok(mut pool) => {
            // Allocate region on CPU
            println!("ðŸ“¦ Allocating region on CPU...");
            let cpu_region = pool.allocate_optimal(1024 * 1024, Device::Cpu)?;
            println!("   - CPU region: {} bytes", cpu_region.size);
            
            // Demonstrate transfer to same device (should be cached)
            println!("ðŸ”„ Transferring to same device (CPU -> CPU)...");
            let start = Instant::now();
            let transferred_region = pool.transfer_to_device(&cpu_region, Device::Cpu)?;
            let transfer_time = start.elapsed();
            
            println!("   - Transfer completed in {:?}", transfer_time);
            println!("   - Same device transfer uses caching for efficiency");
            
            // Get transfer statistics
            if let Ok(stats) = pool.get_comprehensive_stats() {
                println!("   - Cross-backend transfers: {}", stats.global_stats.cross_backend_transfers);
                println!("   - Cache hit rate: {:.1}%", stats.global_stats.cache_hit_rate);
            }
            
            // Clean up
            pool.deallocate_region(cpu_region)?;
            pool.deallocate_region(transferred_region)?;
            
            println!("âœ… Memory transfer demonstration completed");
        }
        Err(e) => println!("âŒ Memory transfer demo failed: {}", e),
    }
    
    Ok(())
}

/// Demonstrate performance impact of memory optimizations
async fn demonstrate_performance_impact() -> Result<()> {
    println!("âš¡ Analyzing performance impact of memory optimizations...");
    
    // Create inference engine with optimized memory management
    let config = EngineConfig {
        optimization_level: OptimizationLevel::Aggressive,
        batch_size: 32,
        ..Default::default()
    };
    
    println!("ðŸš€ Creating optimized inference engine...");
    let engine_result = InferenceEngine::with_config(config).await;
    
    match engine_result {
        Ok(_engine) => {
            println!("âœ… Optimized inference engine created successfully");
            
            // Simulate inference operations to measure memory impact
            println!("ðŸ”¬ Simulating inference operations...");
            
            let tensor_sizes = vec![1024, 4096, 16384, 65536];
            let mut total_allocation_time = std::time::Duration::new(0, 0);
            
            for (i, size) in tensor_sizes.iter().enumerate() {
                println!("   - Test {}: Creating tensor of size {} elements", i + 1, size);
                
                let start = Instant::now();
                
                // Create test tensor (this will use optimized memory allocation)
                let tensor_result = Tensor::zeros(&[*size], DType::F32, &Device::Cpu);
                
                match tensor_result {
                    Ok(_tensor) => {
                        let allocation_time = start.elapsed();
                        total_allocation_time += allocation_time;
                        
                        println!("     âœ… Allocated in {:?}", allocation_time);
                    }
                    Err(e) => {
                        println!("     âŒ Allocation failed: {}", e);
                    }
                }
            }
            
            println!("ðŸ“Š Performance Summary:");
            println!("   - Total allocation time: {:?}", total_allocation_time);
            println!("   - Average allocation time: {:?}", 
                     total_allocation_time / tensor_sizes.len() as u32);
            
            // Memory usage analysis
            println!("ðŸ“ˆ Memory Usage Analysis:");
            println!("   - Enhanced memory pool provides:");
            println!("     â€¢ Reduced fragmentation through intelligent allocation");
            println!("     â€¢ Cross-backend caching for transfer optimization");  
            println!("     â€¢ Adaptive allocation strategies for different workloads");
            println!("     â€¢ GPU buffer pool management for Metal/MLX backends");
            
        }
        Err(e) => println!("âŒ Optimized inference engine creation failed: {}", e),
    }
    
    Ok(())
}

/// Helper function to format memory sizes
fn format_memory_size(bytes: usize) -> String {
    if bytes >= 1024 * 1024 * 1024 {
        format!("{:.2} GB", bytes as f64 / (1024.0 * 1024.0 * 1024.0))
    } else if bytes >= 1024 * 1024 {
        format!("{:.2} MB", bytes as f64 / (1024.0 * 1024.0))
    } else if bytes >= 1024 {
        format!("{:.2} KB", bytes as f64 / 1024.0)
    } else {
        format!("{} bytes", bytes)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_gpu_memory_manager_creation() {
        let result = GPUMemoryManager::new(Device::Cpu);
        assert!(result.is_ok());
    }
    
    #[tokio::test]
    async fn test_enhanced_memory_pool() {
        let pool_result = EnhancedMemoryPool::new(Device::Cpu, 1024 * 1024, 512 * 1024);
        assert!(pool_result.is_ok());
        
        let mut pool = pool_result.unwrap();
        let region = pool.allocate_optimal(1024, Device::Cpu);
        assert!(region.is_ok());
    }
    
    #[test]
    fn test_memory_size_formatting() {
        assert_eq!(format_memory_size(1024), "1.00 KB");
        assert_eq!(format_memory_size(1024 * 1024), "1.00 MB");
        assert_eq!(format_memory_size(1024 * 1024 * 1024), "1.00 GB");
    }
}
