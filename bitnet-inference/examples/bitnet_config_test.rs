//! Test BitNet layer configuration extraction from GGUF files
//!
//! This example demonstrates the extraction of BitNet-specific model configuration
//! parameters from GGUF metadata for proper layer construction.

use bitnet_inference::{GgufLoader, BitNetModelConfig, Result};
use std::path::Path;

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging to see configuration extraction details
    tracing_subscriber::fmt::init();

    println!("BitNet Layer Configuration Extraction Test");
    println!("==========================================");

    // Test configuration extraction functionality
    test_configuration_extraction().await?;

    println!("\nüéâ Configuration extraction test completed successfully!");
    Ok(())
}

/// Test BitNet configuration extraction
async fn test_configuration_extraction() -> Result<()> {
    println!("\nüî¨ Testing BitNet Configuration Extraction");
    
    let loader = GgufLoader::new();
    
    // Test with default GGUF metadata (simulated)
    println!("Testing configuration extraction with default values...");
    
    // Note: In a real scenario, we would test with an actual GGUF file:
    // let config = loader.extract_model_config("path/to/microsoft/bitnet-b1.58-2B-4T.gguf").await?;
    
    // For demonstration, create a sample configuration
    let sample_config = create_sample_bitnet_config();
    
    println!("‚úÖ Sample BitNet Configuration:");
    print_configuration_summary(&sample_config);
    
    // Validate the configuration
    match sample_config.validate() {
        Ok(()) => println!("‚úÖ Configuration validation: PASSED"),
        Err(e) => println!("‚ùå Configuration validation: FAILED - {}", e),
    }
    
    // Test configuration helpers
    test_configuration_helpers(&sample_config);
    
    Ok(())
}

/// Create a sample BitNet configuration for testing
fn create_sample_bitnet_config() -> BitNetModelConfig {
    use bitnet_inference::{BasicModelInfo, LayerConfig, AttentionConfig, 
                          NormalizationConfig, BitLinearConfig, TokenizerConfig, RopeConfig};
    
    BitNetModelConfig {
        basic_info: BasicModelInfo {
            name: "microsoft/bitnet-b1.58-2B-4T".to_string(),
            architecture: "bitnet-b1.58".to_string(),
            version: "1.0".to_string(),
            parameter_count: 2_000_000_000, // 2B parameters
            context_length: 4096,
        },
        layer_config: LayerConfig {
            n_layers: 32,
            hidden_size: 2048,
            intermediate_size: 8192, // 4 * hidden_size
            model_dim: 2048,
        },
        attention_config: AttentionConfig {
            n_heads: 32,
            n_kv_heads: Some(32), // Same as n_heads for standard attention
            head_dim: 64, // hidden_size / n_heads = 2048 / 32 = 64
            max_seq_len: 4096,
            rope_config: RopeConfig {
                rope_freq_base: 10000.0,
                rope_scaling: None,
                rope_dim: 64,
            },
        },
        normalization_config: NormalizationConfig {
            rms_norm_eps: 1e-5,
            use_bias: false,
        },
        bitlinear_config: BitLinearConfig {
            weight_bits: 2, // 1.58-bit quantization
            activation_bits: 8,
            use_weight_scaling: true,
            use_activation_scaling: true,
            quantization_scheme: "1.58bit".to_string(),
        },
        tokenizer_config: TokenizerConfig {
            vocab_size: 128256, // LLaMA 3 vocabulary
            tokenizer_type: "llama3".to_string(),
            bos_token_id: Some(1),
            eos_token_id: Some(2),
            pad_token_id: None,
        },
        extra_metadata: std::collections::HashMap::new(),
    }
}

/// Print a summary of the extracted configuration
fn print_configuration_summary(config: &BitNetModelConfig) {
    println!("  üìã Basic Info:");
    println!("    ‚Ä¢ Name: {}", config.basic_info.name);
    println!("    ‚Ä¢ Architecture: {}", config.basic_info.architecture);
    println!("    ‚Ä¢ Parameters: {:.2}B", config.basic_info.parameter_count as f64 / 1e9);
    println!("    ‚Ä¢ Context Length: {}", config.basic_info.context_length);
    
    println!("  üèóÔ∏è  Layer Configuration:");
    println!("    ‚Ä¢ Layers: {}", config.layer_config.n_layers);
    println!("    ‚Ä¢ Hidden Size: {}", config.layer_config.hidden_size);
    println!("    ‚Ä¢ Intermediate Size: {}", config.layer_config.intermediate_size);
    
    println!("  üîç Attention Configuration:");
    println!("    ‚Ä¢ Heads: {}", config.attention_config.n_heads);
    println!("    ‚Ä¢ Head Dimension: {}", config.attention_config.head_dim);
    println!("    ‚Ä¢ Max Sequence Length: {}", config.attention_config.max_seq_len);
    println!("    ‚Ä¢ RoPE Base Frequency: {}", config.attention_config.rope_config.rope_freq_base);
    
    println!("  üìè Normalization:");
    println!("    ‚Ä¢ RMSNorm Epsilon: {}", config.normalization_config.rms_norm_eps);
    println!("    ‚Ä¢ Use Bias: {}", config.normalization_config.use_bias);
    
    println!("  ‚ö° BitLinear Configuration:");
    println!("    ‚Ä¢ Weight Bits: {}", config.bitlinear_config.weight_bits);
    println!("    ‚Ä¢ Activation Bits: {}", config.bitlinear_config.activation_bits);
    println!("    ‚Ä¢ Quantization Scheme: {}", config.bitlinear_config.quantization_scheme);
    
    println!("  üî§ Tokenizer:");
    println!("    ‚Ä¢ Vocabulary Size: {}", config.tokenizer_config.vocab_size);
    println!("    ‚Ä¢ Type: {}", config.tokenizer_config.tokenizer_type);
}

/// Test configuration helper methods
fn test_configuration_helpers(config: &BitNetModelConfig) {
    println!("\nüß™ Testing Configuration Helpers:");
    
    // Test calculated head dimension
    let calculated_head_dim = config.calculated_head_dim();
    println!("  ‚Ä¢ Calculated Head Dim: {} (expected: {})", 
             calculated_head_dim, config.attention_config.head_dim);
    
    if calculated_head_dim == config.attention_config.head_dim {
        println!("    ‚úÖ Head dimension calculation: CORRECT");
    } else {
        println!("    ‚ùå Head dimension calculation: INCORRECT");
    }
    
    // Test grouped-query attention detection
    let uses_gqa = config.uses_grouped_query_attention();
    println!("  ‚Ä¢ Uses Grouped-Query Attention: {}", uses_gqa);
    
    // Test effective key-value heads
    let effective_kv_heads = config.effective_n_kv_heads();
    println!("  ‚Ä¢ Effective KV Heads: {}", effective_kv_heads);
    
    // Test some inference-ready calculations
    println!("\nüìä Inference-Ready Calculations:");
    println!("  ‚Ä¢ Total Parameters: {:.2}B", config.basic_info.parameter_count as f64 / 1e9);
    println!("  ‚Ä¢ Memory per Head: {:.2}KB", 
             (config.attention_config.head_dim * config.basic_info.context_length * 4) as f64 / 1024.0);
    println!("  ‚Ä¢ Total Attention Memory: {:.2}MB", 
             (config.attention_config.n_heads * config.attention_config.head_dim * 
              config.basic_info.context_length * 4) as f64 / (1024.0 * 1024.0));
}