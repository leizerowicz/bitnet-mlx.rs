//! Comprehensive Batch Processing Tests
//!
//! This module contains extensive tests for all batch processing functionality
//! in BitNet Core, focusing on memory conversion batch processing since the
//! sequence module is not currently exported.

use bitnet_core::device::get_cpu_device;
use bitnet_core::memory::{
    conversion::{
        batch::BatchConverter, config::BatchConfig, ConversionContext, ConversionQuality,
        ConversionStrategy, Converter,
    },
    tensor::{BitNetDType, BitNetTensor},
    HybridMemoryPool,
};
use std::sync::Arc;
use std::time::Instant;

#[cfg(test)]
mod memory_conversion_batch_tests {
    use super::*;

    fn setup_test_environment() -> (Arc<HybridMemoryPool>, candle_core::Device) {
        let pool = Arc::new(HybridMemoryPool::new().unwrap());
        let device = get_cpu_device();
        (pool, device)
    }

    #[test]
    fn test_batch_converter_creation() {
        let config = BatchConfig::default();
        let _converter = BatchConverter::new(config).unwrap();

        // Test default configuration
        let _default_converter = BatchConverter::default().unwrap();

        // Test that converter supports basic conversions
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F16,
            get_cpu_device(),
            get_cpu_device(),
            vec![2, 2],
        );
        assert!(_default_converter.supports(&context));
    }

    #[test]
    fn test_empty_batch_conversion() {
        let (pool, _device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        let tensors: Vec<BitNetTensor> = vec![];
        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        assert!(results.is_empty());
    }

    #[test]
    fn test_single_tensor_batch_conversion() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        let tensor = BitNetTensor::ones(&[2, 2], BitNetDType::F32, &device, &pool).unwrap();
        let tensors = vec![tensor.clone()];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        assert_eq!(results.len(), 1);
        assert_eq!(results[0].dtype(), BitNetDType::F16);
        assert_eq!(results[0].shape(), tensor.shape());
    }

    #[test]
    fn test_multiple_tensor_batch_conversion() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        let tensors = vec![
            BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[3, 3], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::zeros(&[4, 4], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        let original_shapes: Vec<_> = tensors.iter().map(|t| t.shape()).collect();
        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        assert_eq!(results.len(), 3);

        for (i, result) in results.iter().enumerate() {
            assert_eq!(result.dtype(), BitNetDType::F16);
            assert_eq!(result.shape(), original_shapes[i]);
        }
    }

    #[test]
    fn test_mixed_batch_conversion() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        let conversions = vec![
            (
                BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
                BitNetDType::F16,
            ),
            (
                BitNetTensor::ones(&[3, 3], BitNetDType::F32, &device, &pool).unwrap(),
                BitNetDType::I8,
            ),
            (
                BitNetTensor::zeros(&[4, 4], BitNetDType::F16, &device, &pool).unwrap(),
                BitNetDType::I8,
            ),
        ];

        let results = converter.batch_convert_mixed(&conversions, &pool).unwrap();

        assert_eq!(results.len(), 3);
        assert_eq!(results[0].dtype(), BitNetDType::F16);
        assert_eq!(results[1].dtype(), BitNetDType::I8);
        assert_eq!(results[2].dtype(), BitNetDType::I8);

        // Verify shapes are preserved
        for (i, (original_tensor, _)) in conversions.iter().enumerate() {
            assert_eq!(results[i].shape(), original_tensor.shape());
        }
    }

    #[test]
    fn test_batch_conversion_order_preservation() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Create tensors with different sizes to ensure they get grouped differently
        let tensors = vec![
            BitNetTensor::zeros(&[1, 1], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[10, 10], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[20, 20], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        // Verify order is preserved despite potential grouping
        assert_eq!(results.len(), 4);
        assert_eq!(results[0].shape(), vec![1, 1]);
        assert_eq!(results[1].shape(), vec![10, 10]);
        assert_eq!(results[2].shape(), vec![2, 2]);
        assert_eq!(results[3].shape(), vec![20, 20]);
    }

    #[test]
    fn test_parallel_batch_processing() {
        let (pool, device) = setup_test_environment();

        // Create config with parallel processing enabled
        let mut config = BatchConfig::default();
        config.enable_parallel_processing = true;
        config.batch_worker_threads = 2;

        let converter = BatchConverter::new(config).unwrap();

        // Create enough tensors to potentially trigger parallel processing
        let tensors = vec![
            BitNetTensor::zeros(&[5, 5], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[5, 5], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::zeros(&[6, 6], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[6, 6], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::zeros(&[7, 7], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[7, 7], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        assert_eq!(results.len(), 6);
        for (i, result) in results.iter().enumerate() {
            assert_eq!(result.dtype(), BitNetDType::F16);
            assert_eq!(result.shape(), tensors[i].shape());
        }
    }

    #[test]
    fn test_mixed_parallel_processing() {
        let (pool, device) = setup_test_environment();

        let mut config = BatchConfig::default();
        config.enable_parallel_processing = true;
        config.batch_worker_threads = 2;

        let converter = BatchConverter::new(config).unwrap();

        let conversions = vec![
            (
                BitNetTensor::zeros(&[4, 4], BitNetDType::F32, &device, &pool).unwrap(),
                BitNetDType::F16,
            ),
            (
                BitNetTensor::ones(&[4, 4], BitNetDType::F32, &device, &pool).unwrap(),
                BitNetDType::F16,
            ),
            (
                BitNetTensor::zeros(&[5, 5], BitNetDType::F16, &device, &pool).unwrap(),
                BitNetDType::I8,
            ),
            (
                BitNetTensor::ones(&[5, 5], BitNetDType::F16, &device, &pool).unwrap(),
                BitNetDType::I8,
            ),
        ];

        let results = converter.batch_convert_mixed(&conversions, &pool).unwrap();

        assert_eq!(results.len(), 4);
        assert_eq!(results[0].dtype(), BitNetDType::F16);
        assert_eq!(results[1].dtype(), BitNetDType::F16);
        assert_eq!(results[2].dtype(), BitNetDType::I8);
        assert_eq!(results[3].dtype(), BitNetDType::I8);
    }

    #[test]
    fn test_batch_conversion_with_different_dtypes() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Mix of different source data types (using only supported conversions)
        let tensors = vec![
            BitNetTensor::zeros(&[3, 3], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[3, 3], BitNetDType::F16, &device, &pool).unwrap(),
            // Use F32 instead of I8 to avoid unsupported conversion
            BitNetTensor::zeros(&[3, 3], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        assert_eq!(results.len(), 3);
        for result in &results {
            assert_eq!(result.dtype(), BitNetDType::F16);
            assert_eq!(result.shape(), vec![3, 3]);
        }
    }

    #[test]
    fn test_batch_conversion_performance_comparison() {
        let (pool, device) = setup_test_environment();

        // Test both sequential and parallel processing
        let mut sequential_config = BatchConfig::default();
        sequential_config.enable_parallel_processing = false;

        let mut parallel_config = BatchConfig::default();
        parallel_config.enable_parallel_processing = true;
        parallel_config.batch_worker_threads = 2;

        let sequential_converter = BatchConverter::new(sequential_config).unwrap();
        let parallel_converter = BatchConverter::new(parallel_config).unwrap();

        // Create a reasonable number of tensors for timing
        let tensors: Vec<BitNetTensor> = (0..10)
            .map(|_| BitNetTensor::zeros(&[10, 10], BitNetDType::F32, &device, &pool).unwrap())
            .collect();

        // Time sequential processing
        let start = Instant::now();
        let sequential_results = sequential_converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        let sequential_time = start.elapsed();

        // Time parallel processing
        let start = Instant::now();
        let parallel_results = parallel_converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        let parallel_time = start.elapsed();

        // Verify results are equivalent
        assert_eq!(sequential_results.len(), parallel_results.len());
        for (seq, par) in sequential_results.iter().zip(parallel_results.iter()) {
            assert_eq!(seq.dtype(), par.dtype());
            assert_eq!(seq.shape(), par.shape());
        }

        println!("Sequential time: {sequential_time:?}, Parallel time: {parallel_time:?}");

        // Note: Parallel might not always be faster for small batches due to overhead
        // This test mainly ensures both approaches work correctly
    }

    #[test]
    fn test_batch_converter_error_handling() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Test with valid tensor (this should succeed)
        let tensor = BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap();
        let tensors = vec![tensor];

        // This should succeed for valid conversions
        let results = converter.batch_convert(&tensors, BitNetDType::F16, &pool);
        assert!(results.is_ok());

        // Test empty mixed conversions
        let empty_conversions: Vec<(BitNetTensor, BitNetDType)> = vec![];
        let mixed_results = converter
            .batch_convert_mixed(&empty_conversions, &pool)
            .unwrap();
        assert!(mixed_results.is_empty());
    }

    #[test]
    fn test_batch_size_limits() {
        let (pool, device) = setup_test_environment();

        let mut config = BatchConfig::default();
        config.max_batch_size = 2; // Very small batch size

        let converter = BatchConverter::new(config).unwrap();

        // Create more tensors than the batch size limit
        let tensors = vec![
            BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::zeros(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[2, 2], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        // Should still process all tensors correctly
        assert_eq!(results.len(), 4);
        for result in &results {
            assert_eq!(result.dtype(), BitNetDType::F16);
        }
    }
}

#[cfg(test)]
mod edge_cases_and_error_tests {
    use super::*;

    fn setup_test_environment() -> (Arc<HybridMemoryPool>, candle_core::Device) {
        let pool = Arc::new(HybridMemoryPool::new().unwrap());
        let device = get_cpu_device();
        (pool, device)
    }

    #[test]
    fn test_memory_conversion_with_zero_sized_tensors() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Create tensor with zero dimension
        let tensor = BitNetTensor::zeros(&[0, 5], BitNetDType::F32, &device, &pool).unwrap();
        let tensors = vec![tensor];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        assert_eq!(results.len(), 1);
        assert_eq!(results[0].shape(), vec![0, 5]);
    }

    #[test]
    fn test_batch_conversion_same_dtype() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Convert F32 to F32 (no-op conversion)
        let tensor = BitNetTensor::ones(&[3, 3], BitNetDType::F32, &device, &pool).unwrap();
        let tensors = vec![tensor.clone()];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F32, &pool)
            .unwrap();
        assert_eq!(results.len(), 1);
        assert_eq!(results[0].dtype(), BitNetDType::F32);
        assert_eq!(results[0].shape(), tensor.shape());
    }

    #[test]
    fn test_conversion_context_creation_and_properties() {
        let device = get_cpu_device();

        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F16,
            device.clone(),
            device.clone(),
            vec![2, 3],
        );

        assert_eq!(context.sourcedtype, BitNetDType::F32);
        assert_eq!(context.targetdtype, BitNetDType::F16);
        assert_eq!(context.shape, vec![2, 3]);
        assert_eq!(context.strategy, ConversionStrategy::Auto);
        assert_eq!(context.quality, ConversionQuality::Balanced);
        assert!(context.preserve_metadata);
    }

    #[test]
    fn test_conversion_context_strategy_selection() {
        let device = get_cpu_device();

        // Zero-copy case (same type)
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F32,
            device.clone(),
            device.clone(),
            vec![2, 3],
        );
        assert!(context.is_zero_copy_compatible());
        assert_eq!(context.optimal_strategy(), ConversionStrategy::ZeroCopy);

        // In-place case (smaller target)
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F16,
            device.clone(),
            device.clone(),
            vec![2, 3],
        );
        assert!(context.is_in_place_compatible());
        assert_eq!(context.optimal_strategy(), ConversionStrategy::InPlace);

        // Large tensor streaming case
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::I8,
            device.clone(),
            device.clone(),
            vec![10000, 10000], // Large tensor
        );
        // The actual strategy might be InPlace if the conversion is compatible
        let strategy = context.optimal_strategy();
        assert!(
            strategy == ConversionStrategy::Streaming || strategy == ConversionStrategy::InPlace
        );
    }

    #[test]
    fn test_conversion_context_memory_overhead() {
        let device = get_cpu_device();

        // Zero-copy should have no overhead
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F32,
            device.clone(),
            device.clone(),
            vec![100, 100],
        );
        assert_eq!(context.memory_overhead_bytes(), 0);

        // Standard conversion should have target size overhead
        let context = ConversionContext::new(
            BitNetDType::F16,
            BitNetDType::F32,
            device.clone(),
            device.clone(),
            vec![100, 100],
        );
        let expected_overhead = BitNetDType::F32.bytes_for_elements(10000);
        assert_eq!(context.memory_overhead_bytes(), expected_overhead);
    }

    #[test]
    fn test_conversion_context_with_custom_settings() {
        let device = get_cpu_device();

        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F16,
            device.clone(),
            device.clone(),
            vec![2, 3],
        )
        .with_strategy(ConversionStrategy::Streaming)
        .with_quality(ConversionQuality::Precise)
        .with_preserve_metadata(false);

        assert_eq!(context.strategy, ConversionStrategy::Streaming);
        assert_eq!(context.quality, ConversionQuality::Precise);
        assert!(!context.preserve_metadata);
    }
}

#[cfg(test)]
mod performance_and_memory_tests {
    use super::*;

    fn setup_test_environment() -> (Arc<HybridMemoryPool>, candle_core::Device) {
        let pool = Arc::new(HybridMemoryPool::new().unwrap());
        let device = get_cpu_device();
        (pool, device)
    }

    #[test]
    fn test_large_batch_processing_performance() {
        let (pool, device) = setup_test_environment();

        // Test with larger batches to evaluate performance characteristics
        let mut config = BatchConfig::default();
        config.max_batch_size = 100;
        config.enable_parallel_processing = true;
        config.batch_worker_threads = 4;

        let converter = BatchConverter::new(config).unwrap();

        // Create a substantial number of tensors
        let tensors: Vec<BitNetTensor> = (0..50)
            .map(|_| BitNetTensor::zeros(&[8, 8], BitNetDType::F32, &device, &pool).unwrap())
            .collect();

        let start = Instant::now();
        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        let duration = start.elapsed();

        assert_eq!(results.len(), 50);

        // Performance should be reasonable (this is a rough check)
        assert!(
            duration.as_millis() < 5000,
            "Batch processing took too long: {duration:?}"
        );

        println!("Processed {} tensors in {:?}", tensors.len(), duration);
    }

    #[test]
    fn test_memory_pressure_handling() {
        let (pool, device) = setup_test_environment();

        // Configure with very small batch sizes to simulate memory pressure
        let mut config = BatchConfig::default();
        config.max_batch_size = 2;
        config.enable_parallel_processing = false; // Avoid parallel overhead

        let converter = BatchConverter::new(config).unwrap();

        // Create tensors of varying sizes
        let tensors = vec![
            BitNetTensor::zeros(&[100, 100], BitNetDType::F32, &device, &pool).unwrap(), // Large
            BitNetTensor::ones(&[10, 10], BitNetDType::F32, &device, &pool).unwrap(),    // Small
            BitNetTensor::zeros(&[50, 50], BitNetDType::F32, &device, &pool).unwrap(),   // Medium
            BitNetTensor::ones(&[200, 200], BitNetDType::F32, &device, &pool).unwrap(), // Very large
        ];

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        // Should handle all tensors despite memory constraints
        assert_eq!(results.len(), 4);

        // Verify shapes and types are preserved
        for (i, result) in results.iter().enumerate() {
            assert_eq!(result.dtype(), BitNetDType::F16);
            assert_eq!(result.shape(), tensors[i].shape());
        }
    }

    #[test]
    fn test_batch_processing_scalability() {
        let (pool, device) = setup_test_environment();

        // Test scalability with different batch sizes
        let batch_sizes = vec![1, 5, 10, 20];
        let tensor_count = 20;

        for batch_size in batch_sizes {
            let mut config = BatchConfig::default();
            config.max_batch_size = batch_size;
            config.enable_parallel_processing = batch_size > 5; // Enable parallel for larger batches

            let converter = BatchConverter::new(config).unwrap();

            let tensors: Vec<BitNetTensor> = (0..tensor_count)
                .map(|_| BitNetTensor::zeros(&[5, 5], BitNetDType::F32, &device, &pool).unwrap())
                .collect();

            let start = Instant::now();
            let results = converter
                .batch_convert(&tensors, BitNetDType::F16, &pool)
                .unwrap();
            let duration = start.elapsed();

            assert_eq!(results.len(), tensor_count);

            println!("Batch size {batch_size}: processed {tensor_count} tensors in {duration:?}");

            // Verify all conversions are correct
            for result in &results {
                assert_eq!(result.dtype(), BitNetDType::F16);
                assert_eq!(result.shape(), vec![5, 5]);
            }
        }
    }

    #[test]
    fn test_converter_trait_implementation() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Test single tensor conversion through Converter trait
        let tensor = BitNetTensor::zeros(&[4, 4], BitNetDType::F32, &device, &pool).unwrap();
        let context = ConversionContext::new(
            BitNetDType::F32,
            BitNetDType::F16,
            device.clone(),
            device.clone(),
            vec![4, 4],
        );

        let result = converter.convert(&tensor, &context, &pool).unwrap();
        assert_eq!(result.dtype(), BitNetDType::F16);
        assert_eq!(result.shape(), vec![4, 4]);

        // Test supports method
        assert!(converter.supports(&context));

        // Test time estimation
        let estimated_time = converter.estimate_time_ms(&context);
        assert!(estimated_time > 0);
    }
}

#[cfg(test)]
mod integration_tests {
    use super::*;

    fn setup_test_environment() -> (Arc<HybridMemoryPool>, candle_core::Device) {
        let pool = Arc::new(HybridMemoryPool::new().unwrap());
        let device = get_cpu_device();
        (pool, device)
    }

    #[test]
    fn test_end_to_end_tensor_conversion_pipeline() {
        let (pool, device) = setup_test_environment();

        // Simulate a complete tensor processing pipeline
        let mut config = BatchConfig::default();
        config.enable_parallel_processing = true;
        config.batch_worker_threads = 2;
        config.max_batch_size = 8;
        config.sort_by_size = true;

        let converter = BatchConverter::new(config).unwrap();

        // Create mixed tensor types and sizes (simulating real ML workload)
        let tensors = vec![
            // Small tensors (embeddings)
            BitNetTensor::zeros(&[128], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[256], BitNetDType::F32, &device, &pool).unwrap(),
            // Medium tensors (hidden states)
            BitNetTensor::zeros(&[32, 768], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[16, 768], BitNetDType::F32, &device, &pool).unwrap(),
            // Large tensors (weight matrices)
            BitNetTensor::zeros(&[768, 512], BitNetDType::F32, &device, &pool).unwrap(),
            BitNetTensor::ones(&[512, 768], BitNetDType::F32, &device, &pool).unwrap(),
            // Mixed precision inputs (using supported types)
            BitNetTensor::zeros(&[64, 64], BitNetDType::F16, &device, &pool).unwrap(),
            BitNetTensor::ones(&[32, 32], BitNetDType::F32, &device, &pool).unwrap(),
        ];

        // Test batch conversion to different target types
        let f16_results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();
        let i8_results = converter
            .batch_convert(&tensors, BitNetDType::I8, &pool)
            .unwrap();

        // Verify all conversions
        assert_eq!(f16_results.len(), tensors.len());
        assert_eq!(i8_results.len(), tensors.len());

        for (i, (f16_result, i8_result)) in f16_results.iter().zip(i8_results.iter()).enumerate() {
            assert_eq!(f16_result.shape(), tensors[i].shape());
            assert_eq!(i8_result.shape(), tensors[i].shape());
            assert_eq!(f16_result.dtype(), BitNetDType::F16);
            assert_eq!(i8_result.dtype(), BitNetDType::I8);
        }

        // Test mixed batch conversion
        let mixed_conversions: Vec<(BitNetTensor, BitNetDType)> = tensors
            .iter()
            .enumerate()
            .map(|(i, tensor)| {
                let target_dtype = if i % 2 == 0 {
                    BitNetDType::F16
                } else {
                    BitNetDType::I8
                };
                (tensor.clone(), target_dtype)
            })
            .collect();

        let mixed_results = converter
            .batch_convert_mixed(&mixed_conversions, &pool)
            .unwrap();
        assert_eq!(mixed_results.len(), tensors.len());

        for (i, result) in mixed_results.iter().enumerate() {
            let expected_dtype = if i % 2 == 0 {
                BitNetDType::F16
            } else {
                BitNetDType::I8
            };
            assert_eq!(result.dtype(), expected_dtype);
            assert_eq!(result.shape(), tensors[i].shape());
        }
    }

    #[test]
    fn test_batch_config_validation() {
        // Test valid config
        let valid_config = BatchConfig::default();
        let converter = BatchConverter::new(valid_config);
        assert!(converter.is_ok());

        // Test config with custom settings
        let mut custom_config = BatchConfig::default();
        custom_config.max_batch_size = 64;
        custom_config.enable_parallel_processing = true;
        custom_config.batch_worker_threads = 8;
        custom_config.sort_by_size = true;

        let converter = BatchConverter::new(custom_config);
        assert!(converter.is_ok());
    }

    #[test]
    fn test_comprehensive_dtype_conversions() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Test all major dtype conversions
        let source_dtypes = vec![BitNetDType::F32, BitNetDType::F16, BitNetDType::I8];
        let target_dtypes = vec![BitNetDType::F32, BitNetDType::F16, BitNetDType::I8];

        for source_dtype in &source_dtypes {
            for target_dtype in &target_dtypes {
                let tensor = match source_dtype {
                    BitNetDType::F32 => {
                        BitNetTensor::ones(&[4, 4], *source_dtype, &device, &pool).unwrap()
                    }
                    BitNetDType::F16 => {
                        BitNetTensor::zeros(&[4, 4], *source_dtype, &device, &pool).unwrap()
                    }
                    BitNetDType::I8 => {
                        BitNetTensor::ones(&[4, 4], *source_dtype, &device, &pool).unwrap()
                    }
                    _ => continue, // Skip unsupported dtypes for this test
                };

                let tensors = vec![tensor];
                let results = converter.batch_convert(&tensors, *target_dtype, &pool);

                // Verify conversion succeeded or failed appropriately
                match results {
                    Ok(converted) => {
                        assert_eq!(converted.len(), 1);
                        assert_eq!(converted[0].dtype(), *target_dtype);
                        assert_eq!(converted[0].shape(), vec![4, 4]);
                        println!("Successfully converted {source_dtype:?} to {target_dtype:?}");
                    }
                    Err(e) => {
                        println!("Conversion from {source_dtype:?} to {target_dtype:?} failed (expected for some combinations): {e}");
                    }
                }
            }
        }
    }

    #[test]
    fn test_batch_processing_with_memory_pool_integration() {
        let (pool, device) = setup_test_environment();
        let converter = BatchConverter::default().unwrap();

        // Get initial memory metrics
        let initial_metrics = pool.get_metrics();

        // Create and process a batch of tensors
        let tensors: Vec<BitNetTensor> = (0..5)
            .map(|_| BitNetTensor::zeros(&[16, 16], BitNetDType::F32, &device, &pool).unwrap())
            .collect();

        let results = converter
            .batch_convert(&tensors, BitNetDType::F16, &pool)
            .unwrap();

        // Verify results
        assert_eq!(results.len(), 5);
        for result in &results {
            assert_eq!(result.dtype(), BitNetDType::F16);
            assert_eq!(result.shape(), vec![16, 16]);
        }

        // Check that memory was allocated for the conversions
        let final_metrics = pool.get_metrics();
        assert!(final_metrics.total_allocated >= initial_metrics.total_allocated);

        println!(
            "Memory allocated during batch processing: {} bytes",
            final_metrics.total_allocated - initial_metrics.total_allocated
        );
    }
}
