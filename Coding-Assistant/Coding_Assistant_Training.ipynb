{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlEDiDSSc6wD"
      },
      "source": [
        "# BitNet-Rust AI Model Finetuning Notebook\n",
        "\n",
        "This notebook collects data as specified in the AI Model Training Research Guide for the BitNet-Rust project.\n",
        "It uses Crawl4AI for web scraping, allows uploading a codebase, prepares a dataset, and finetunes Gemma-3-4B using Unsloth.\n",
        "After finetuning, the model is tested and can be exported for Ollama.\n",
        "\n",
        "**Note:** This notebook is designed to run on Google Colab with GPU enabled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mAHJYQDc6wE"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Installing all required packages. This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mkj5rXoc6wE"
      },
      "outputs": [],
      "source": [
        "# Install core dependencies\n",
        "!pip install crawl4ai gitpython transformers torch datasets huggingface_hub trl peft accelerate bitsandbytes PyPDF2 requests\n",
        "\n",
        "# Install Unsloth for fast training\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install git-lfs for large file support\n",
        "!apt install git-lfs -y\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqeQIocac6wE"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz0kCmibc6wF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import git\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "import asyncio\n",
        "import json\n",
        "import requests\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "from datasets import Dataset\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"üìö All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOXNx5z7c6wF"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Define what data sources to collect from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eovDaI_c6wF"
      },
      "outputs": [],
      "source": [
        "# Repositories to clone\n",
        "repos = {\n",
        "    'bitnet_rust': 'https://github.com/ocentra/bitnet.rs',\n",
        "    'mlx': 'https://github.com/ml-explore/mlx'\n",
        "}\n",
        "\n",
        "# URLs to crawl for documentation and tutorials\n",
        "urls_to_crawl = [\n",
        "    'https://doc.rust-lang.org/book/',  # Rust Book\n",
        "    'https://developer.apple.com/metal/',  # Metal Programming Guide\n",
        "    'https://users.rust-lang.org/',  # Rust Users Forum\n",
        "]\n",
        "\n",
        "# Research papers to download\n",
        "papers = [\n",
        "    'https://arxiv.org/pdf/2310.11453'  # BitNet paper\n",
        "]\n",
        "\n",
        "print(f\"üìã Configured {len(repos)} repos, {len(urls_to_crawl)} URLs, and {len(papers)} papers to process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoFgs6Rc6wF"
      },
      "source": [
        "## Step 1: Clone Repositories and Extract Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8qd6V6Pc6wF"
      },
      "outputs": [],
      "source": [
        "data_dir = 'training_data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "def clone_repo(url, name):\n",
        "    \"\"\"Clone a repository and return its path\"\"\"\n",
        "    path = os.path.join(data_dir, name)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"üîÑ Cloning {name} from {url}...\")\n",
        "        git.Repo.clone_from(url, path)\n",
        "    else:\n",
        "        print(f\"‚úÖ {name} already exists, skipping clone\")\n",
        "    return path\n",
        "\n",
        "def extract_code_files(path):\n",
        "    \"\"\"Extract relevant files from repository\"\"\"\n",
        "    texts = []\n",
        "    file_extensions = ('.rs', '.md', '.toml', '.py', '.cpp', '.h')\n",
        "\n",
        "    for root, _, files in os.walk(path):\n",
        "        # Skip hidden directories and common ignore patterns\n",
        "        if any(skip in root for skip in ['.git', 'target', '__pycache__', 'node_modules']):\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(file_extensions):\n",
        "                try:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                        if content.strip():  # Only add non-empty files\n",
        "                            texts.append(content)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Could not read {file}: {e}\")\n",
        "    return texts\n",
        "\n",
        "# Process repositories\n",
        "repo_texts = []\n",
        "for name, url in repos.items():\n",
        "    try:\n",
        "        path = clone_repo(url, name)\n",
        "        texts = extract_code_files(path)\n",
        "        repo_texts.extend(texts)\n",
        "        print(f\"üìÅ Extracted {len(texts)} files from {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {name}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total files extracted from repositories: {len(repo_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ekiDDzvc6wF"
      },
      "source": [
        "## Step 2: Scrape Web Pages with Crawl4AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6UGUHoJc6wF"
      },
      "outputs": [],
      "source": [
        "async def crawl_url_safe(crawler, url):\n",
        "    \"\"\"Safely crawl a single URL with error handling\"\"\"\n",
        "    try:\n",
        "        print(f\"üï∑Ô∏è Crawling {url}...\")\n",
        "        result = await crawler.arun(url=url)\n",
        "        if result.success and result.markdown:\n",
        "            print(f\"‚úÖ Successfully crawled {url}\")\n",
        "            return result.markdown\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: No content from {url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error crawling {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def crawl_all_urls():\n",
        "    \"\"\"Crawl all configured URLs\"\"\"\n",
        "    async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "        results = []\n",
        "        for url in urls_to_crawl:\n",
        "            content = await crawl_url_safe(crawler, url)\n",
        "            if content:\n",
        "                results.append(content)\n",
        "        return results\n",
        "\n",
        "# Run the crawler\n",
        "web_texts = await crawl_all_urls()\n",
        "print(f\"\\n‚úÖ Successfully scraped {len(web_texts)} web pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acWJQsrlc6wG"
      },
      "source": [
        "## Step 3: Download and Extract Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKu_M-xhc6wG"
      },
      "outputs": [],
      "source": [
        "def download_and_extract_pdf(url):\n",
        "    \"\"\"Download PDF and extract text content\"\"\"\n",
        "    try:\n",
        "        print(f\"üìÑ Downloading PDF from {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save temporarily\n",
        "        temp_file = 'temp_paper.pdf'\n",
        "        with open(temp_file, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # Extract text\n",
        "        with open(temp_file, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = ''\n",
        "            for page_num, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    text += page.extract_text() + '\\n'\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Could not extract page {page_num}: {e}\")\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(temp_file)\n",
        "\n",
        "        if text.strip():\n",
        "            print(f\"‚úÖ Successfully extracted text from PDF ({len(text)} characters)\")\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: No text extracted from PDF\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing PDF from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process papers\n",
        "paper_texts = []\n",
        "for url in papers:\n",
        "    content = download_and_extract_pdf(url)\n",
        "    if content:\n",
        "        paper_texts.append(content)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully processed {len(paper_texts)} papers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj1Ayb7pc6wG"
      },
      "source": [
        "## Step 4: Upload Your Codebase\n",
        "\n",
        "Upload a zip file containing your codebase to include in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVeru5tWc6wG"
      },
      "outputs": [],
      "source": [
        "print('üìÅ Please upload your codebase as a zip file:')\n",
        "print('   - The zip should contain your source code files')\n",
        "print('   - Supported file types: .rs, .md, .toml, .py, .cpp, .h')\n",
        "print('   - Large files (>100MB) may take time to process')\n",
        "\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        codebase_zip = list(uploaded.keys())[0]\n",
        "        print(f\"üì¶ Processing uploaded file: {codebase_zip}\")\n",
        "\n",
        "        # Extract zip\n",
        "        !unzip -q \"{codebase_zip}\" -d codebase\n",
        "\n",
        "        # Extract code files\n",
        "        codebase_texts = extract_code_files('codebase')\n",
        "        print(f\"‚úÖ Extracted {len(codebase_texts)} files from uploaded codebase\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No file uploaded, continuing without user codebase\")\n",
        "        codebase_texts = []\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error processing upload: {e}\")\n",
        "    print(\"Continuing without user codebase...\")\n",
        "    codebase_texts = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9DUQaIc6wG"
      },
      "source": [
        "## Step 5: Prepare Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is8EFNymc6wG"
      },
      "outputs": [],
      "source": [
        "# Combine all collected data\n",
        "all_texts = repo_texts + web_texts + paper_texts + codebase_texts\n",
        "\n",
        "# Filter out empty or very short texts\n",
        "filtered_texts = [text for text in all_texts if len(text.strip()) > 50]\n",
        "\n",
        "print(f\"üìä Dataset Statistics:\")\n",
        "print(f\"   - Repository files: {len(repo_texts)}\")\n",
        "print(f\"   - Web pages: {len(web_texts)}\")\n",
        "print(f\"   - Research papers: {len(paper_texts)}\")\n",
        "print(f\"   - User codebase: {len(codebase_texts)}\")\n",
        "print(f\"   - Total samples: {len(filtered_texts)}\")\n",
        "\n",
        "if not filtered_texts:\n",
        "    raise ValueError(\"‚ùå No training data collected! Please check your data sources.\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_dict({'text': filtered_texts})\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Split for evaluation\n",
        "if len(filtered_texts) > 10:\n",
        "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    print(f\"üìà Split into {len(dataset['train'])} training and {len(dataset['test'])} evaluation samples\")\n",
        "else:\n",
        "    # Too few samples for split\n",
        "    dataset = {'train': dataset, 'test': dataset}\n",
        "    print(f\"‚ö†Ô∏è Using same data for training and evaluation due to small dataset size\")\n",
        "\n",
        "print(\"‚úÖ Dataset prepared successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xApE-XuAc6wG"
      },
      "source": [
        "## Step 6: Setup Model for Fine-tuning\n",
        "\n",
        "**Important:** You need to log in to Hugging Face to download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75kiXtxsc6wG"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "print(\"üîê Please log in to Hugging Face to download the model:\")\n",
        "login()\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto-detect based on GPU\n",
        "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
        "\n",
        "print(\"ü§ñ Loading Gemma-3-4B model...\")\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"google/gemma-2-2b-it\",  # Using 2B model as it's more stable\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"This might be due to:\")\n",
        "    print(\"  - GPU memory limitations\")\n",
        "    print(\"  - Hugging Face authentication issues\")\n",
        "    print(\"  - Network connectivity problems\")\n",
        "    raise\n",
        "\n",
        "# Setup LoRA adapter for efficient fine-tuning\n",
        "print(\"‚öôÔ∏è Setting up LoRA adapter...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXApPActc6wH"
      },
      "source": [
        "## Step 7: Fine-tune the Model\n",
        "\n",
        "This will take some time depending on your data size and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVSpArvAc6wH"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting fine-tuning process...\")\n",
        "print(\"This may take 10-60 minutes depending on data size and GPU speed.\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=100,  # Adjust based on dataset size\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        eval_steps=25,\n",
        "        save_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"./training_output\",\n",
        "        report_to=\"none\",  # Disable wandb logging\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "print(\"üéâ Fine-tuning completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1hr3DBc6wH"
      },
      "source": [
        "## Step 8: Test the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVbfyW43c6wH"
      },
      "outputs": [],
      "source": [
        "print(\"üß™ Testing the fine-tuned model...\")\n",
        "\n",
        "# Prepare model for inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"What is BitNet and how does it work?\",\n",
        "    \"Explain Rust memory management\",\n",
        "    \"How do you implement a neural network in Rust?\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüî§ Test {i}: {prompt}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Remove the original prompt from response\n",
        "        response = response[len(prompt):].strip()\n",
        "        print(f\"üìù Response: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating response: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model testing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjcW9V57c6wH"
      },
      "source": [
        "## Step 9: Save and Export Model\n",
        "\n",
        "Save the fine-tuned model for later use or export to Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35SwnAoqc6wH"
      },
      "outputs": [],
      "source": [
        "print(\"üíæ Saving fine-tuned model...\")\n",
        "\n",
        "# Save the model\n",
        "output_dir = \"bitnet_rust_model\"\n",
        "try:\n",
        "    model.save_pretrained_merged(\n",
        "        output_dir,\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\"\n",
        "    )\n",
        "    print(f\"‚úÖ Model saved to '{output_dir}' directory\")\n",
        "\n",
        "    # List saved files\n",
        "    print(\"\\nüìÇ Saved files:\")\n",
        "    for file in os.listdir(output_dir):\n",
        "        file_path = os.path.join(output_dir, file)\n",
        "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
        "        print(f\"   - {file} ({size:.1f} MB)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving model: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéä FINE-TUNING COMPLETE! üéä\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"\\n1. üì• Download your model:\")\n",
        "print(\"   - Go to Files tab in Colab\")\n",
        "print(f\"   - Download the '{output_dir}' folder\")\n",
        "\n",
        "print(\"\\n2. ü¶ô For Ollama integration:\")\n",
        "print(\"   - Install llama.cpp: git clone https://github.com/ggerganov/llama.cpp\")\n",
        "print(\"   - Convert to GGUF format:\")\n",
        "print(f\"     python llama.cpp/convert.py {output_dir} --outtype q8_0 --outfile bitnet_rust.gguf\")\n",
        "print(\"   - Create Ollama Modelfile and import\")\n",
        "\n",
        "print(\"\\n3. üöÄ Use your model:\")\n",
        "print(\"   - Load in your applications\")\n",
        "print(\"   - Deploy to production\")\n",
        "print(\"   - Continue fine-tuning with more data\")\n",
        "\n",
        "print(\"\\n‚ú® Happy coding with your BitNet-Rust AI assistant!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}